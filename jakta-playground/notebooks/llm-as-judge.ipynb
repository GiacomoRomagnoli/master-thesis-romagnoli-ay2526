{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.models import GeminiModel\n",
    "from deepeval.test_case import LLMTestCaseParams, LLMTestCase\n",
    "from deepeval.evaluate import CacheConfig\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Optional, Dict, Any\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b82c22-f141-47ca-9429-c3326a7db619",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Holds a single evaluation result with score and reasoning.\"\"\"\n",
    "    eval_id: str\n",
    "    score: float\n",
    "    reason: str\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "    def save_to_json(self, filename: str) -> None:\n",
    "        \"\"\"Save results to JSON file.\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=4)\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResults:\n",
    "    \"\"\"Holds all model results for the entire experiment.\"\"\"\n",
    "    results: List[EvaluationResult]\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert to dictionary for JSON serialization.\"\"\"\n",
    "        return {\n",
    "            'results': [res.to_dict() for res in self.results]\n",
    "        }\n",
    "    \n",
    "    def save_to_json(self, filename: str) -> None:\n",
    "        \"\"\"Save results to JSON file.\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59cc000e-bded-487b-9f04-97c3feb09d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_contents(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def file_content_iterator(root_dir, whitelist):\n",
    "    for provider in os.listdir(root_dir):\n",
    "        provider_path = os.path.join(root_dir, provider)\n",
    "        if not os.path.isdir(provider_path):\n",
    "            continue\n",
    "\n",
    "        for model_name in os.listdir(provider_path):\n",
    "            key = f\"{provider}/{model_name}\"\n",
    "            if key in whitelist:\n",
    "                model_path = os.path.join(provider_path, model_name)\n",
    "                if not os.path.isdir(model_path):\n",
    "                    continue\n",
    "    \n",
    "                txt_files = glob.glob(os.path.join(model_path, \"generation_result_*.txt\"))\n",
    "                for txt_file in txt_files:\n",
    "                    filename = os.path.basename(txt_file)\n",
    "                    content = read_file_contents(txt_file)\n",
    "                    yield provider, model_name, filename, content\n",
    "             \n",
    "def run_eval(root_dir, whitelist):\n",
    "    results = []\n",
    "\n",
    "    for provider, model, filename, content in file_content_iterator(root_dir, whitelist):\n",
    "        print(f\"Evaluating: {provider}/{model}/{filename}\")\n",
    "        test_case = LLMTestCase(\n",
    "            input=\"\",\n",
    "            actual_output=content,\n",
    "            expected_output=ground_truth\n",
    "        )\n",
    "        raw_eval_result = evaluate(\n",
    "            test_cases=[test_case],\n",
    "            metrics=[bdi_plan_correctness],\n",
    "            #show_indicator=False,\n",
    "            #display=None,\n",
    "            #print_results=False\n",
    "        )\n",
    "        eval_id = f\"{provider}/{model}\"\n",
    "        eval_result_dict = raw_eval_result.test_results[0].metrics_data[0]\n",
    "        eval_result = EvaluationResult(\n",
    "            eval_id=eval_id,\n",
    "            score=eval_result_dict.score,\n",
    "            reason=eval_result_dict.reason,\n",
    "        )\n",
    "        results.append(eval_result)\n",
    "    return results\n",
    "\n",
    "def load_whitelist(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return set(line.strip() for line in f if line.strip() and not line.startswith('#'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc30d8d5-5ced-457f-bf88-d2146cb35fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load variables from .env file into the environment\n",
    "load_dotenv()\n",
    "\n",
    "ground_truth = \"\"\"\n",
    "EVENT: achieve reach(Object)\n",
    "CONDITIONS:\n",
    "  - there_is(Object, here)\n",
    "OPERATIONS:\n",
    "  - <none>\n",
    "\n",
    "EVENT: achieve reach(Object)\n",
    "CONDITIONS:\n",
    "  - there_is(Object, Direction)\n",
    "OPERATIONS:\n",
    "  - execute move(Direction)\n",
    "\n",
    "EVENT: achieve reach(Object)\n",
    "CONDITIONS:\n",
    "  - not(there_is(Object, _))\n",
    "OPERATIONS:\n",
    "  - execute getDirectionToMove(Direction)\n",
    "  - execute move(Direction)\n",
    "  - achieve reach(Object)\n",
    "\"\"\"\n",
    "\n",
    "judge_deep_eval = GeminiModel(\n",
    "    model_name=\"gemini-2.5-pro\", # gemini-2.5-flash\n",
    "    api_key=os.environ.get(\"GOOGLE_API_KEY\"),\n",
    ")\n",
    "\n",
    "bdi_plan_correctness = GEval(\n",
    "    name=\"BDI Plan Correctness and Minimality\",\n",
    "    evaluation_steps=[\n",
    "        \"Extract invented goals, beliefs, and plans from the 'actual output'.\",\n",
    "        \"Compare extracted plans against 'expected output' plans for logical equivalence and coverage.\",\n",
    "        \"Assess if invented goals/beliefs are necessary or add needless complexity compared to 'expected output'.\",\n",
    "        \"Evaluate plan minimality; penalize unnecessary subgoals, conditions, or operations vs 'expected output'.\",\n",
    "        \"Verify that operations correctly use specified prefixes (execute, achieve, add, etc.) and admissible actions.\",\n",
    "        \"Check if conditions logically correspond to the intended plan activation scenario.\",\n",
    "        \"Score based on plan correctness, necessity of inventions, and adherence to minimality principle.\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model=judge_deep_eval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "id": "890b3ef1-1976-4c5f-a8b8-e0b48dd1ffd6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "root_dir = \"../metrics\"\n",
    "\n",
    "whitelist_filename = \"whitelist.txt\"\n",
    "whitelist = load_whitelist(whitelist_filename)\n",
    "\n",
    "evals = run_eval(root_dir, whitelist)\n",
    "ExperimentResults(evals).save_to_json(\"result.json\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
